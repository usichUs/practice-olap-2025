import requests
import re
import time
import csv
import pandas as pd
from collections import defaultdict, Counter
from datetime import datetime

class HHEnhancedParser:
    def __init__(self):
        self.base_url = "https://api.hh.ru"
        self.headers = {'User-Agent': 'HH-User-Agent'}
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∫–∞—Ä—Ç–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π —Å –ø—Ä–∏–≤—è–∑–∫–æ–π –∫ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è–º –§–ì–û–°/–ü—Ä–æ—Ñ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤
        self.tech_competency_mapping = {
            # –ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ
            'Python': {
                'category': '–Ø–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è',
                'fgos_competencies': ['–ü–ö-1', '–ü–ö-2', '–ü–ö-3'],
                'prof_standards': ['06.001_A', '06.001_B'],
                'level': 'middle',
                'domain': 'backend'
            },
            'JavaScript': {
                'category': '–Ø–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è', 
                'fgos_competencies': ['–ü–ö-1', '–ü–ö-2'],
                'prof_standards': ['06.001_A'],
                'level': 'middle',
                'domain': 'frontend'
            },
            'React': {
                'category': '–§—Ä–µ–π–º–≤–æ—Ä–∫',
                'fgos_competencies': ['–ü–ö-1', '–ü–ö-2'],
                'prof_standards': ['06.001_A'],
                'level': 'middle',
                'domain': 'frontend'
            },
            'SQL': {
                'category': '–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö',
                'fgos_competencies': ['–ü–ö-2', '–ü–ö-3'],
                'prof_standards': ['06.001_B', '06.022_A'],
                'level': 'basic',
                'domain': 'data'
            },
            'Docker': {
                'category': 'DevOps',
                'fgos_competencies': ['–ü–ö-3', '–ü–ö-4'],
                'prof_standards': ['06.001_C'],
                'level': 'advanced',
                'domain': 'infrastructure'
            },
            'Git': {
                'category': '–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç',
                'fgos_competencies': ['–ü–ö-1'],
                'prof_standards': ['06.001_A'],
                'level': 'basic',
                'domain': 'development'
            }
        }
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–æ–ª–µ–π
        self.role_keywords = {
            'backend': ['backend', '–±—ç–∫–µ–Ω–¥', '—Å–µ—Ä–≤–µ—Ä–Ω–∞—è', 'api', '–º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å'],
            'frontend': ['frontend', '—Ñ—Ä–æ–Ω—Ç–µ–Ω–¥', 'ui', 'ux', '–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å'],
            'fullstack': ['fullstack', '—Ñ—É–ª—Å—Ç–µ–∫', '–ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª'],
            'data': ['data', '–¥–∞–Ω–Ω—ã–µ', '–∞–Ω–∞–ª–∏—Ç–∏–∫', 'bi', 'etl'],
            'devops': ['devops', '–¥–µ–æ–ø—Å', '–∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞', '–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä'],
            'mobile': ['mobile', '–º–æ–±–∏–ª—å–Ω', 'android', 'ios', 'flutter'],
            'qa': ['qa', '—Ç–µ—Å—Ç–∏—Ä–æ–≤—â–∏–∫', '—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ', '–∞–≤—Ç–æ—Ç–µ—Å—Ç']
        }
        
        # –£—Ä–æ–≤–Ω–∏ –æ–ø—ã—Ç–∞
        self.experience_mapping = {
            '–ù–µ—Ç –æ–ø—ã—Ç–∞': 'junior',
            '–û—Ç 1 –≥–æ–¥–∞ –¥–æ 3 –ª–µ—Ç': 'junior',
            '–û—Ç 3 –¥–æ 6 –ª–µ—Ç': 'middle', 
            '–ë–æ–ª–µ–µ 6 –ª–µ—Ç': 'senior'
        }

    def search_vacancies_enhanced(self, queries, max_pages=3):
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π"""
        all_vacancies = []
        
        for query in queries:
            print(f"üîç –ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: {query}")
            
            for page in range(max_pages):
                params = {
                    'text': query,
                    'page': page,
                    'per_page': 100,
                    'area': 1,  # –ú–æ—Å–∫–≤–∞
                    'only_with_salary': 'false'
                }
                
                try:
                    response = requests.get(f"{self.base_url}/vacancies", 
                                          params=params, headers=self.headers)
                    data = response.json()
                    
                    vacancies = data.get('items', [])
                    if not vacancies:
                        break
                        
                    # –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å—Ä–∞–∑—É
                    detailed_vacancies = self.get_detailed_vacancies(vacancies)
                    all_vacancies.extend(detailed_vacancies)
                    
                    print(f"  –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page + 1}: {len(detailed_vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π")
                    time.sleep(0.5)
                    
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞: {e}")
                    break
        
        return all_vacancies

    def get_detailed_vacancies(self, vacancy_list):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –≤–∞–∫–∞–Ω—Å–∏—è–º"""
        detailed = []
        
        for vacancy in vacancy_list:
            try:
                # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                vacancy_id = vacancy['id']
                
                # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
                response = requests.get(f"{self.base_url}/vacancies/{vacancy_id}",
                                      headers=self.headers)
                full_data = response.json()
                
                processed_vacancy = self.process_vacancy_data(full_data)
                detailed.append(processed_vacancy)
                
                time.sleep(0.2)  # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∞–∫–∞–Ω—Å–∏–∏ {vacancy.get('id')}: {e}")
                continue
        
        return detailed

    def process_vacancy_data(self, vacancy_data):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–∏ —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –≤—Å–µ–π –Ω—É–∂–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        
        # –ë–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        processed = {
            'vacancy_id': vacancy_data.get('id'),
            'title': vacancy_data.get('name'),
            'company': vacancy_data.get('employer', {}).get('name'),
            'company_size': self.get_company_size(vacancy_data.get('employer', {})),
            'area': vacancy_data.get('area', {}).get('name'),
            'published_date': vacancy_data.get('published_at'),
            'experience_raw': vacancy_data.get('experience', {}).get('name'),
            'experience_level': self.map_experience_level(vacancy_data.get('experience', {}).get('name')),
            'schedule': vacancy_data.get('schedule', {}).get('name'),
            'employment': vacancy_data.get('employment', {}).get('name'),
        }
        
        # –ó–∞—Ä–ø–ª–∞—Ç–∞
        salary = vacancy_data.get('salary')
        if salary:
            processed.update({
                'salary_from': salary.get('from'),
                'salary_to': salary.get('to'),
                'salary_currency': salary.get('currency'),
                'salary_gross': salary.get('gross'),
                'avg_salary': self.calculate_avg_salary(salary)
            })
        
        # –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        description = vacancy_data.get('description', '') or ''
        key_skills = vacancy_data.get('key_skills', [])
        skills_text = ' '.join([skill.get('name', '') for skill in key_skills])
        
        full_text = f"{processed['title']} {description} {skills_text}"
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        technologies = self.extract_technologies_detailed(full_text)
        processed['technologies'] = technologies
        processed['tech_count'] = len(technologies)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–æ–ª–∏/–¥–æ–º–µ–Ω–∞
        processed['role'] = self.determine_role(full_text)
        processed['domain'] = self.determine_domain(full_text)
        
        # –ö–ª—é—á–µ–≤—ã–µ –Ω–∞–≤—ã–∫–∏
        processed['key_skills'] = [skill.get('name') for skill in key_skills]
        processed['skills_count'] = len(key_skills)
        
        # –°–≤—è–∑—å —Å –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è–º–∏ –§–ì–û–°/–ü—Ä–æ—Ñ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤
        competencies = self.map_to_competencies(technologies)
        processed['fgos_competencies'] = competencies['fgos']
        processed['prof_standard_competencies'] = competencies['prof_standards']
        
        return processed

    def extract_technologies_detailed(self, text):
        """–î–µ—Ç–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π —Å –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π"""
        if not text:
            return {}
        
        text_lower = text.lower()
        found_technologies = {}
        
        for tech, info in self.tech_competency_mapping.items():
            pattern = r'\b' + re.escape(tech.lower()) + r'\b'
            matches = len(re.findall(pattern, text_lower))
            
            if matches > 0:
                found_technologies[tech] = {
                    'frequency': matches,
                    'category': info['category'],
                    'level': info['level'],
                    'domain': info['domain'],
                    'fgos_competencies': info['fgos_competencies'],
                    'prof_standards': info['prof_standards']
                }
        
        return found_technologies

    def map_to_competencies(self, technologies):
        """–°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π —Å –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è–º–∏"""
        fgos_competencies = set()
        prof_standards = set()
        
        for tech, info in technologies.items():
            fgos_competencies.update(info.get('fgos_competencies', []))
            prof_standards.update(info.get('prof_standards', []))
        
        return {
            'fgos': list(fgos_competencies),
            'prof_standards': list(prof_standards)
        }

    def determine_role(self, text):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–æ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞"""
        text_lower = text.lower()
        role_scores = defaultdict(int)
        
        for role, keywords in self.role_keywords.items():
            for keyword in keywords:
                role_scores[role] += len(re.findall(r'\b' + re.escape(keyword) + r'\b', text_lower))
        
        return max(role_scores, key=role_scores.get) if role_scores else 'general'

    def determine_domain(self, text):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏"""
        domain_keywords = {
            'fintech': ['–±–∞–Ω–∫', '—Ñ–∏–Ω–∞–Ω—Å', '–ø–ª–∞—Ç–µ–∂', '–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç'],
            'ecommerce': ['–∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω', 'ecommerce', '—Ç–æ—Ä–≥–æ–≤–ª'],
            'gamedev': ['–∏–≥—Ä', '–≥–µ–π–º–¥–µ–≤', 'unity', 'unreal'],
            'edtech': ['–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ', '–æ–±—É—á–µ–Ω–∏–µ', '–∫—É—Ä—Å'],
            'healthtech': ['–º–µ–¥–∏—Ü–∏–Ω', '–∑–¥–æ—Ä–æ–≤—å–µ', '–∫–ª–∏–Ω–∏–∫'],
            'government': ['–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤', '–≥–æ—Å—É—Å–ª—É–≥', '–±—é–¥–∂–µ—Ç']
        }
        
        text_lower = text.lower()
        domain_scores = defaultdict(int)
        
        for domain, keywords in domain_keywords.items():
            for keyword in keywords:
                domain_scores[domain] += len(re.findall(r'\b' + re.escape(keyword) + r'\b', text_lower))
        
        return max(domain_scores, key=domain_scores.get) if domain_scores else 'general'

    def get_company_size(self, employer_data):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –∫–æ–º–ø–∞–Ω–∏–∏"""
        if not employer_data:
            return None
        
        # –ú–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å –ª–æ–≥–∏–∫—É –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞
        name = employer_data.get('name', '').lower()
        if any(word in name for word in ['—è–Ω–¥–µ–∫—Å', '—Å–±–µ—Ä', 'mail', 'vk']):
            return 'large'
        return 'unknown'

    def map_experience_level(self, experience_raw):
        """–ú–∞–ø–ø–∏–Ω–≥ —É—Ä–æ–≤–Ω—è –æ–ø—ã—Ç–∞"""
        return self.experience_mapping.get(experience_raw, 'unknown')

    def calculate_avg_salary(self, salary_data):
        """–†–∞—Å—á–µ—Ç —Å—Ä–µ–¥–Ω–µ–π –∑–∞—Ä–ø–ª–∞—Ç—ã"""
        if not salary_data:
            return None
        
        salary_from = salary_data.get('from')
        salary_to = salary_data.get('to')
        
        if salary_from and salary_to:
            return (salary_from + salary_to) / 2
        return salary_from or salary_to

    def save_enhanced_csv(self, processed_vacancies, timestamp=None):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ CSV"""
        if not timestamp:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 1. –û—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏
        vacancies_file = f'csv_for_export/hh_vacancies_enhanced_{timestamp}.csv'
        
        vacancies_data = []
        for vacancy in processed_vacancies:
            row = {
                'vacancy_id': vacancy['vacancy_id'],
                'title': vacancy['title'],
                'company': vacancy['company'],
                'company_size': vacancy.get('company_size'),
                'area': vacancy['area'],
                'published_date': vacancy['published_date'],
                'experience_raw': vacancy['experience_raw'],
                'experience_level': vacancy['experience_level'],
                'role': vacancy['role'],
                'domain': vacancy['domain'],
                'salary_from': vacancy.get('salary_from'),
                'salary_to': vacancy.get('salary_to'),
                'avg_salary': vacancy.get('avg_salary'),
                'tech_count': vacancy['tech_count'],
                'skills_count': vacancy['skills_count'],
                'fgos_competencies_count': len(vacancy['fgos_competencies']),
                'prof_competencies_count': len(vacancy['prof_standard_competencies'])
            }
            vacancies_data.append(row)
        
        pd.DataFrame(vacancies_data).to_csv(vacancies_file, index=False, encoding='utf-8')
        
        # 2. –î–µ—Ç–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π
        tech_file = f'csv_for_export/hh_technologies_detailed_{timestamp}.csv'
        tech_data = []
        
        for vacancy in processed_vacancies:
            for tech, info in vacancy['technologies'].items():
                tech_data.append({
                    'vacancy_id': vacancy['vacancy_id'],
                    'technology': tech,
                    'frequency': info['frequency'],
                    'category': info['category'],
                    'level': info['level'],
                    'domain': info['domain'],
                    'fgos_competencies': ','.join(info['fgos_competencies']),
                    'prof_standards': ','.join(info['prof_standards'])
                })
        
        pd.DataFrame(tech_data).to_csv(tech_file, index=False, encoding='utf-8')
        
        # 3. –°–≤–æ–¥–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞
        analytics_file = f'csv_for_export/hh_analytics_{timestamp}.csv'
        self.create_analytics_summary(processed_vacancies, analytics_file)
        
        print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã —Ñ–∞–π–ª—ã:")
        print(f"  üìÑ –í–∞–∫–∞–Ω—Å–∏–∏: {vacancies_file}")
        print(f"  üîß –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏: {tech_file}")
        print(f"  üìä –ê–Ω–∞–ª–∏—Ç–∏–∫–∞: {analytics_file}")

    def create_analytics_summary(self, processed_vacancies, filename):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–≤–æ–¥–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏"""
        analytics = []
        
        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º
        tech_stats = defaultdict(lambda: {
            'total_mentions': 0,
            'vacancy_count': 0,
            'avg_salary': [],
            'roles': defaultdict(int),
            'experience_levels': defaultdict(int),
            'domains': defaultdict(int)
        })
        
        for vacancy in processed_vacancies:
            for tech, info in vacancy['technologies'].items():
                stats = tech_stats[tech]
                stats['total_mentions'] += info['frequency']
                stats['vacancy_count'] += 1
                
                if vacancy.get('avg_salary'):
                    stats['avg_salary'].append(vacancy['avg_salary'])
                
                stats['roles'][vacancy['role']] += 1
                stats['experience_levels'][vacancy['experience_level']] += 1
                stats['domains'][vacancy['domain']] += 1
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏
        for tech, stats in tech_stats.items():
            analytics.append({
                'technology': tech,
                'total_mentions': stats['total_mentions'],
                'vacancy_count': stats['vacancy_count'],
                'avg_salary': sum(stats['avg_salary']) / len(stats['avg_salary']) if stats['avg_salary'] else None,
                'top_role': max(stats['roles'], key=stats['roles'].get) if stats['roles'] else None,
                'top_experience': max(stats['experience_levels'], key=stats['experience_levels'].get) if stats['experience_levels'] else None,
                'top_domain': max(stats['domains'], key=stats['domains'].get) if stats['domains'] else None
            })
        
        pd.DataFrame(analytics).to_csv(filename, index=False, encoding='utf-8')

    def run_enhanced_parsing(self):
        """–ó–∞–ø—É—Å–∫ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        print("üöÄ –ó–∞–ø—É—Å–∫ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ HH")
        
        # –ü–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        queries = [
            "python —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫",
            "backend —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫", 
            "frontend —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫",
            "fullstack —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫",
            "—Å–∏—Å—Ç–µ–º–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫",
            "data analyst",
            "devops –∏–Ω–∂–µ–Ω–µ—Ä"
        ]
        
        # –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
        all_vacancies = self.search_vacancies_enhanced(queries, max_pages=2)
        
        print(f"\nüìä –°–æ–±—Ä–∞–Ω–æ {len(all_vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.save_enhanced_csv(all_vacancies, timestamp)
        
        return all_vacancies

if __name__ == "__main__":
    parser = HHEnhancedParser()
    vacancies = parser.run_enhanced_parsing()